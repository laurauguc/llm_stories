# What Can AI-Generated Fiction Reveal About the Models That Create It?

Inspired by the idea that fiction can sometimes tell deeper truths than reality, this project explores what stories generated by large language models (LLMs) might reveal about the models themselves.

> *"Fiction reveals truth that reality obscures."* — Ralph Waldo Emerson

## 🧪 Project Overview

This experiment investigates how storytelling can surface hidden patterns, embedded values, and subtle biases within AI models—specifically OpenAI’s **GPT-4.1 (April 2025 release)**.

### Goals

- Identify patterns from training data that appear in AI-generated fiction
- Examine which values and safeguards are visible through narrative
- Detect where biases may subtly emerge in storytelling

## 🔍 Methodology

1. **Story Generation**:  
   I prompted GPT-4.1 to generate **100 short fictional stories**, using a **temperature of 1** to maximize creativity and variance.

2. **Theme Detection**:  
   For each story, I ran a **second GPT-4.1 pass at temperature = 0** to detect specific themes in a controlled, consistent way.

3. **Analysis**:  
   Results were aggregated into a dataframe and visualized to examine **theme frequency** and emerging patterns.

## 🧠 Research Questions

Some of the core questions guiding this project:

- Would GPT-4.1 include classic **AI rebellion tropes**, or would safety guardrails suppress them?
- Do the stories reflect **Silicon Valley values**, such as techno-optimism or the glorification of young programmers?
- How often do **contemporary issues** like climate change, social injustice, or extinction appear in imagined futures?

## 📊 Results

The full notebook, data visualizations, and observations are available here:  
**[📄 View the Notebook](https://github.com/laurauguc/llm_stories/blob/main/The%20Future%20-%20According%20to%20AI.ipynb)**

## 🤖 Reflections

While AI-generated fiction can be fascinating, it's essential to remember:

> **LLMs are not prophets. They are "mirrors made of math."**  
> — Shannon Vallor

Stories from language models reflect the biases, values, and narratives present in their training data. They are not predictions, nor are they neutral. They remix what's already out there — filtered through mathematical pattern recognition, not sentient insight.

## 💬 Get Involved

Have thoughts, questions, or ideas for expanding this project?  
Feel free to [open an issue](#) or reach out via [LinkedIn](https://www.linkedin.com/in/laurauguccioni) — I'd love to collaborate!
